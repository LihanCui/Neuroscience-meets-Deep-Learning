{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from scipy.stats.mstats import zscore\n",
    "import functools\n",
    "# import multiprocessing\n",
    "\n",
    "participants = []\n",
    "for num in xrange(1,10):\n",
    "    participants.append(sio.loadmat(\"./fMRIData/data-science-P\" + str(num) + \".mat\"))\n",
    "\n",
    "def convert(x, data):\n",
    "    if x == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return data[x-1]\n",
    "    \n",
    "def convertMean(x, data, mean_data):\n",
    "    if x != 0:\n",
    "        return data[x-1]\n",
    "    else:\n",
    "        return mean_data\n",
    "\n",
    "processedData = []\n",
    "\n",
    "for i, p in enumerate(participants):\n",
    "    mapping = p[\"meta\"][\"coordToCol\"][0][0]\n",
    "    print 1\n",
    "    for w in xrange(360):\n",
    "        oriData = p[\"data\"][w][0][0]\n",
    "        vConvert = np.vectorize(lambda x: convert(x, oriData))\n",
    "        processedData.append(vConvert(np.copy(mapping)))\n",
    "\n",
    "processedData = np.array(processedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.split(processedData_Mean, 9)\n",
    "\n",
    "for i in xrange(9):\n",
    "    x[i] = zscore(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from scipy.stats.mstats import zscore\n",
    "import functools\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# import multiprocessing\n",
    "\n",
    "participants = []\n",
    "for num in xrange(1,10):\n",
    "    participants.append(sio.loadmat(\"./fMRIData/data-science-P\" + str(num) + \".mat\"))\n",
    "\n",
    "def convert(x, data):\n",
    "    if x == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return data[x-1]\n",
    "    \n",
    "def convertMean(x, data, mean_data):\n",
    "    if x != 0:\n",
    "        return data[x-1]\n",
    "    else:\n",
    "        return mean_data\n",
    "\n",
    "processedData = []\n",
    "\n",
    "for i, p in enumerate(participants):\n",
    "    mapping = p[\"meta\"][\"coordToCol\"][0][0]\n",
    "    print 1\n",
    "    for w in xrange(360):\n",
    "        oriData = p[\"data\"][w][0][0]\n",
    "        vConvert = np.vectorize(lambda x: convert(x, oriData))\n",
    "        processedData.append(vConvert(np.copy(mapping)))\n",
    "        \n",
    "processedData = np.array(processedData)\n",
    "\n",
    "processedData = np.expand_dims(processedData, axis=1) \n",
    "\n",
    "yCond = np.empty([360*9, 1], dtype=\"int32\")\n",
    "yWord = np.empty([360*9, 1], dtype=\"int32\")\n",
    "\n",
    "for i, p in enumerate(participants):\n",
    "    for j in xrange(360):\n",
    "        condN = p[\"info\"][\"cond_number\"][0][j][0]\n",
    "        wordN = p[\"info\"][\"word_number\"][0][j][0]\n",
    "        yCond[i*360+j] = condN\n",
    "        yWord[i*360+j] = 10*condN + wordN\n",
    "\n",
    "yCond = yCond - 2\n",
    "\n",
    "uniqueWord = np.unique(yWord)\n",
    "\n",
    "for i in xrange(360*9):\n",
    "    yWord[i] = np.where(uniqueWord==yWord[i])[0][0]\n",
    "    \n",
    "allYs = np.concatenate((yCond, yWord), axis=1)\n",
    "\n",
    "\n",
    "# flattenProcessedData = processedData.reshape(3240,51*61*23)\n",
    "\n",
    "xTrain, xTestValid, yTrain, yTestValid = \\\n",
    "    train_test_split(processedData, allYs, test_size = 0.3, random_state=44)\n",
    "    \n",
    "xValid, xTest, yValid, yTest = \\\n",
    "    train_test_split(xTestValid, yTestValid, test_size = 0.333, random_state=44)\n",
    "    \n",
    "yTrainCond = yTrain[:,0]\n",
    "yTrainWord = yTrain[:,1]\n",
    "yTestCond = yTest[:,0]\n",
    "yTestWord = yTest[:,1]\n",
    "yValidCond = yValid[:,0]\n",
    "yValidWord = yValid[:,1]\n",
    "    \n",
    "variableList = [xTrain, xTest, yTrain, yTest]\n",
    "data = {\"xTrain\":xTrain, \"xTest\":xTest, \"xValid\":xValid, \"yTrainCond\":yTrainCond, \"yTrainWord\":yTrainWord, \\\n",
    "                  \"yTestCond\":yTestCond, \"yTestWord\":yTestWord, \"yValidCond\":yValidCond, \"yValidWord\":yValidWord}\n",
    "\n",
    "sio.savemat(\"processedData.mat\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the y of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yCond = np.empty([360*9, 1], dtype=\"int32\")\n",
    "yWord = np.empty([360*9, 1], dtype=\"int32\")\n",
    "\n",
    "for i, p in enumerate(participants):\n",
    "    for j in xrange(360):\n",
    "        condN = p[\"info\"][\"cond_number\"][0][j][0]\n",
    "        wordN = p[\"info\"][\"word_number\"][0][j][0]\n",
    "        yCond[i*360+j] = condN\n",
    "        yWord[i*360+j] = 10*condN + wordN\n",
    "\n",
    "yCond = yCond - 2\n",
    "\n",
    "uniqueWord = np.unique(yWord)\n",
    "\n",
    "for i in xrange(360*9):\n",
    "    yWord[i] = np.where(uniqueWord==yWord[i])[0][0]\n",
    "    \n",
    "allYs = np.concatenate((yCond, yWord), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sio.savemat('condOrWords.mat', {\"cond\":yCond, \"word\":yWord})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "truncatedData_Mean = np.array([matrix[:,10:40,20:40,5:15] for matrix in concatenatedData_Mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# flattenProcessedData = processedData.reshape(3240,51*61*23)\n",
    "\n",
    "xTrain, xTestValid, yTrain, yTestValid = \\\n",
    "    train_test_split(lowVarEliminatedData, allYs, test_size = 0.3, random_state=44)\n",
    "    \n",
    "xValid, xTest, yValid, yTest = \\\n",
    "    train_test_split(xTestValid, yTestValid, test_size = 0.333, random_state=44)\n",
    "    \n",
    "yTrainCond = yTrain[:,0]\n",
    "yTrainWord = yTrain[:,1]\n",
    "yTestCond = yTest[:,0]\n",
    "yTestWord = yTest[:,1]\n",
    "yValidCond = yValid[:,0]\n",
    "yValidWord = yValid[:,1]\n",
    "    \n",
    "variableList = [xTrain, xTest, yTrain, yTest]\n",
    "data = {\"xTrain\":xTrain, \"xTest\":xTest, \"xValid\":xValid, \"yTrainCond\":yTrainCond, \"yTrainWord\":yTrainWord, \\\n",
    "                  \"yTestCond\":yTestCond, \"yTestWord\":yTestWord, \"yValidCond\":yValidCond, \"yValidWord\":yValidWord}\n",
    "\n",
    "sio.savemat(\"lowVarEliminatedData.mat\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xValid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import zscore\n",
    "\n",
    "temp = zscore(processedData_Mean[1000,0])\n",
    "\n",
    "temp[30,9,0]\n",
    "processedData_Mean[1000,0,30,9,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "separateX = np.split(processedData, 9)\n",
    "separateY = np.split(allYs, 9)\n",
    "\n",
    "xTrain, xTestValid, yTrain, yTestValid = \\\n",
    "    train_test_split(separateX[0], separateY[0], test_size = 0.3, random_state=44)\n",
    "    \n",
    "xValid, xTest, yValid, yTest = \\\n",
    "    train_test_split(xTestValid, yTestValid, test_size = 0.333, random_state=44)\n",
    "    \n",
    "yTrainCond = yTrain[:,0]\n",
    "yTrainWord = yTrain[:,1]\n",
    "yTestCond = yTest[:,0]\n",
    "yTestWord = yTest[:,1]\n",
    "yValidCond = yValid[:,0]\n",
    "yValidWord = yValid[:,1]\n",
    "    \n",
    "variableList = [xTrain, xTest, yTrain, yTest]\n",
    "data3DCNN_P1_withinclass = {\"xTrain\":xTrain, \"xTest\":xTest, \"xValid\":xValid, \"yTrainCond\":yTrainCond, \"yTrainWord\":yTrainWord, \\\n",
    "                  \"yTestCond\":yTestCond, \"yTestWord\":yTestWord, \"yValidCond\":yValidCond, \"yValidWord\":yValidWord}\n",
    "\n",
    "sio.savemat(\"data3DCNN_P1_withinclass.mat\", data3DCNN_P1_withinclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# separateX = np.split(processedData_Mean, 9)\n",
    "# separateY = np.split(allYs, 9)\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = \\\n",
    "    train_test_split(flattenData, allYs, test_size = 0.2, random_state=44)\n",
    "\n",
    "yTrainCond = yTrain[:,0]\n",
    "yTrainWord = yTrain[:,1]\n",
    "yTestCond = yTest[:,0]\n",
    "yTestWord = yTest[:,1]\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=8)\n",
    "neigh.fit(xTrain, yTrainCond)\n",
    "\n",
    "# lr = LogisticRegression(penalty='l2')\n",
    "# lr = SGDClassifier(loss='hinge', penalty='l1')\n",
    "# lr.fit(xTrain, yTrainCond)\n",
    "\n",
    "# svmClf = svm.SVC(kernel='rbf')\n",
    "# svmClf.fit(xTrain, yTrainCond)\n",
    "\n",
    "# yPredict = neigh.predict(xTest)\n",
    "yPredict = neigh.predict(xTest)\n",
    "# yPredict = svmClf.predict(xTest)\n",
    "\n",
    "print accuracy_score(yTestCond, yPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stds = processedData.reshape(71553,3240).std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattenData = processedData.reshape(3240, 71553)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = sum(i < 0. for i in stds)\n",
    "\n",
    "positions = sorted(range(len(stds)), key=lambda k: stds[k])\n",
    "\n",
    "varHighData = [0]*3240\n",
    "for i, data in enumerate(flattenData):\n",
    "    varHighData[i] = data[positions[count:]+positions[-count:]]\n",
    "    \n",
    "varHighData = np.array(varHighData)\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawData = []\n",
    "for p in participants:\n",
    "    pData = []\n",
    "    \n",
    "    for i in xrange(360):  \n",
    "        pData.append(p[\"data\"][i][0][0])\n",
    "    pData = np.array(pData)\n",
    "        \n",
    "    rawData.append(pData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from numpy import histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = sorted(stds)\n",
    "\n",
    "plt.hist(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sio.savemat(\"stds.mat\", {\"stds\":stds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "truncatedCNN3D = []\n",
    "\n",
    "for i in xrange(3240):\n",
    "    truncatedCNN3D.append(processedData[i,:,23:,:,:])\n",
    "    \n",
    "truncatedCNN3D = np.array(truncatedCNN3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stdWeightedData = stds*processedData\n",
    "stdWeightedDataFlatten = stdWeightedData.reshape(3240, 71553)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import theano\n",
    "from theano import shared\n",
    "from numpy import sqrt, prod, ones, floor, repeat, pi, exp, zeros, sum\n",
    "from conv3d2d import conv3d\n",
    "\n",
    "class ConvVariance(object):\n",
    "    \"\"\" Convolutional layer, Filter Bank Layer \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in_maps, n_out_maps, kernel_shape, video_shape, \n",
    "        batch_size, layer_name=\"Conv\"):\n",
    "\n",
    "\n",
    "        W_shape = (n_out_maps, n_in_maps)+kernel_shape\n",
    "        W_val = np.ones(W_shape, dtype=np.float64)\n",
    "            \n",
    "        self.W = shared(value=W_val, name=layer_name+'_W')\n",
    "\n",
    "        np.ones(W_shape, dtype=np.int)\n",
    "\n",
    "        # 3D convolution; dimshuffle: last 3 dimensions must be (in, h, w)\n",
    "        n_fr, h, w = video_shape\n",
    "        n_fr_k, h_k, w_k = kernel_shape\n",
    "        out = conv3d(\n",
    "                signals=input.dimshuffle([0,2,1,3,4]), \n",
    "                filters=self.W, \n",
    "                signals_shape=(batch_size, n_fr, n_in_maps, h, w), \n",
    "                filters_shape=(n_out_maps, n_fr_k, n_in_maps, h_k, w_k),         \n",
    "                border_mode='valid').dimshuffle([0,2,1,3,4])\n",
    "\n",
    "        self.output = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtensor5 = T.TensorType('float64', (False,)*5)\n",
    "x = dtensor5('x') # the input data\n",
    "\n",
    "layer1 = ConvVariance(x, 1, 1, (10, 10, 10), (51, 61, 23), 3240)\n",
    "\n",
    "blockData = layer1.output.eval({x:processedData})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattendBlockData = blockData.reshape(3240,30576)\n",
    "stdBlock = flattendBlockData.std(0).reshape(42,52,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sio.savemat(\"stds_101010Kernel.mat\", {\"stdBlock\":stdBlock})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "flattenStdBlock = stdBlock.reshape(30576,1)\n",
    "count = 0\n",
    "for i in flattenStdBlock:\n",
    "    if i < 300:\n",
    "        count = count + 1\n",
    "\n",
    "        \n",
    "plt.hist(flattenStdBlock, bins=100)\n",
    "print min(flattenStdBlock)\n",
    "# len(np.select([flattenStdBlock>0], flattenStdBlock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lowVarEliminatedData = np.copy(processedData)\n",
    "\n",
    "for (i, j, k), value in np.ndenumerate(stdBlock):\n",
    "    if value < 100:\n",
    "        lowVarEliminatedData[i:i+10,j:j+10,k:k+10] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattendLowVarEliminatedData = lowVarEliminatedData.reshape(3240,71553)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processedData = np.array(processedData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
